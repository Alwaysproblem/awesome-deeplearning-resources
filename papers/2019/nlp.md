# Nature language process

- BioBERT: pre-trained biomedical language representation model for biomedical text mining. [`arxiv`](https://arxiv.org/abs/1901.08746)
- Cross-lingual Language Model Pretraining. [`arxiv`](https://arxiv.org/abs/1901.07291)
- GILT: Generating Images from Long Text. [`arxiv`](https://arxiv.org/abs/1901.02404)
- Open Research Knowledge Graph: Towards Machine Actionability in Scholarly Communication. [`arxiv`](https://arxiv.org/abs/1901.10816)
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. [`arxiv`](https://arxiv.org/abs/1901.02860) [`code`](https://github.com/kimiyoung/transformer-xl)

## Embedding

- A Multi-Resolution Word Embedding for Document Retrieval from Large Unstructured Knowledge Bases. [`arxiv`](https://arxiv.org/abs/1902.00663)

## Machine Translation

- Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation. [`arxiv`](https://arxiv.org/abs/1902.01509)

## NMT

- Modeling Latent Sentence Structure in Neural Machine Translation. [`arxiv`](https://arxiv.org/abs/1901.06436)

## Text Classification

- Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings. [`arxiv`](https://arxiv.org/abs/1901.07651)
- EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. [`arxiv`](https://arxiv.org/abs/1901.11196) [`code`](https://github.com/jasonwei20/eda_nlp)
